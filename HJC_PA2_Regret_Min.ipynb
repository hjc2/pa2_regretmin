{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjc2/pa2_regretmin/blob/master/HJC_PA2_Regret_Min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e36fda",
      "metadata": {
        "id": "00e36fda"
      },
      "source": [
        "# PA2 ‚Äî Regret Minimization in Repeated Normal-Form Games\n",
        "*Intelligent Agents ‚Äî Programming Assignment 2*\n",
        "\n",
        "**Tasks**\n",
        "1. Compute **average strategy** and **external regret** for the row player from a history of play.\n",
        "2. Implement **Follow-the-Leader (FTL)** against an input column action sequence, and a **self-play** version (both players run FTL).\n",
        "3. Implement **Multiplicative Weights Update (MWU)** against an input column action sequence, and a **self-play** version.\n",
        "4. Plot the **distance to a given NE** for the two **self-play** versions on a specified game.\n",
        "\n",
        "**What to submit:** your completed `.ipynb` on Canvas. Before submitting, do **Runtime ‚Üí Restart and run all**.\n",
        "\n",
        "**Conventions**\n",
        "- Games are two-player normal-form with row payoff matrix `A` (shape `(n_row, n_col)`) and column payoff matrix `B` (same shape).\n",
        "- Action indices are **0-based**.\n",
        "- Use only `numpy` and `matplotlib` (no seaborn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a73bad6",
      "metadata": {
        "id": "2a73bad6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show(arr, name=None):\n",
        "    if name:\n",
        "        print(name, \"=\", end=\" \")\n",
        "    print(np.array(arr))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631754e9",
      "metadata": {
        "id": "631754e9"
      },
      "source": [
        "### üß© Task 1 ‚Äî Average Strategy & External Regret (row player)\n",
        "\n",
        "Implement:\n",
        "\n",
        "- `average_strategy_row(row_actions, n_actions)` ‚Üí empirical distribution over the row player's actions.\n",
        "  - Input: list/array of row action indices in `{0,‚Ä¶,n_actions-1}`\n",
        "  - Output: 1D `np.ndarray` of length `n_actions` summing to 1\n",
        "  - Edge case: if empty, return the **uniform** distribution.\n",
        "\n",
        "- `external_regret_row(row_actions, col_actions, A)` ‚Üí nonnegative float:\n",
        "$$\n",
        "R_T \\,=\\, \\max_{a}\\sum_{t=1}^T A[a, j_t] - \\sum_{t=1}^T A[i_t, j_t].\n",
        "$$\n",
        "- Edge case: if empty, return `0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17449f47",
      "metadata": {
        "id": "17449f47"
      },
      "outputs": [],
      "source": [
        "def average_strategy_row(row_actions, n_actions):\n",
        "    \"\"\"Return the empirical distribution over row actions.\n",
        "    If no actions, return uniform.\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    pass\n",
        "\n",
        "\n",
        "def external_regret_row(row_actions, col_actions, A):\n",
        "    \"\"\"External regret of the row player vs the best fixed row action.\n",
        "    Regret(T) = max_a sum_t A[a, j_t] - sum_t A[i_t, j_t].\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01290cde",
      "metadata": {
        "id": "01290cde"
      },
      "outputs": [],
      "source": [
        "# === Public tests: Task 1 ===\n",
        "def _ok(name, fn):\n",
        "    try:\n",
        "        fn(); print(f\"‚úÖ {name} ‚Äî passed\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"‚ùå {name} ‚Äî {e}\")\n",
        "\n",
        "def test_avg_empty_uniform():\n",
        "    n=3\n",
        "    avg = average_strategy_row([], n)\n",
        "    assert avg.shape==(n,)\n",
        "    assert np.allclose(avg.sum(),1.0)\n",
        "    assert np.allclose(avg, np.ones(n)/n)\n",
        "\n",
        "def test_avg_counts():\n",
        "    avg = average_strategy_row([0,2,2,1,2], 3)\n",
        "    assert np.allclose(avg, [1/5,1/5,3/5])\n",
        "\n",
        "def test_regret_matching_pennies():\n",
        "    A = np.array([[+1,-1],[-1,+1]], float)\n",
        "    row = [0,0,0,0]\n",
        "    col = [1,1,0,1]\n",
        "    reg = external_regret_row(row, col, A)\n",
        "    assert np.isclose(reg, 4.0)\n",
        "\n",
        "_ok(\"T1: avg empty uniform\", test_avg_empty_uniform)\n",
        "_ok(\"T1: avg simple counts\", test_avg_counts)\n",
        "_ok(\"T1: regret (MP)\", test_regret_matching_pennies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36dd3e1a",
      "metadata": {
        "id": "36dd3e1a"
      },
      "source": [
        "### üß© Task 2 ‚Äî Follow-the-Leader (FTL): opponent-given and self-play\n",
        "\n",
        "- `expected_payoff_row_against_mix(A, q)` ‚Üí expected payoffs for each row action vs a column mixed strategy `q`.\n",
        "- `ftl_row(col_actions, A, tie_break='first', seed=None)` ‚Üí row action sequence of length `T` (no peeking at `col_actions[t]` when choosing round `t`).\n",
        "- `ftl_self_play(A, B, T, tie_break='first', seed=None)` ‚Üí simulate both players running FTL **simultaneously** for `T` rounds.\n",
        "  - Returns `(row_actions, col_actions)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf5994c",
      "metadata": {
        "id": "5bf5994c"
      },
      "outputs": [],
      "source": [
        "def expected_payoff_row_against_mix(A, q):\n",
        "    \"\"\"Return v[a] = sum_j A[a,j] * q[j] for each row action a.\"\"\"\n",
        "    # TODO: implement\n",
        "    pass\n",
        "\n",
        "\n",
        "def ftl_row(col_actions, A, tie_break='first', seed=None):\n",
        "    \"\"\"FTL for the row player against realized column actions (no peeking).\"\"\"\n",
        "    # TODO: implement\n",
        "    pass\n",
        "\n",
        "\n",
        "def ftl_self_play(A, B, T, tie_break='first', seed=None):\n",
        "    \"\"\"Self-play: both players run FTL based on the opponent's prefix empirical distribution.\n",
        "    Return (row_actions, col_actions).\"\"\"\n",
        "    # TODO: implement\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbf9f99",
      "metadata": {
        "id": "1dbf9f99"
      },
      "outputs": [],
      "source": [
        "# === Public tests: Task 2 ===\n",
        "def _ok2(name, fn):\n",
        "    try:\n",
        "        fn(); print(f\"‚úÖ {name} ‚Äî passed\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"‚ùå {name} ‚Äî {e}\")\n",
        "\n",
        "def test_expected_row_mix():\n",
        "    A = np.array([[1,0],[0,1]], float)\n",
        "    q = np.array([0.25,0.75])\n",
        "    v = expected_payoff_row_against_mix(A, q)\n",
        "    assert np.allclose(v, [0.25,0.75])\n",
        "\n",
        "def test_ftl_row_basic():\n",
        "    A = np.array([[1,0],[0,1]], float)\n",
        "    col = [1,1,1,1,1]\n",
        "    row = ftl_row(col, A, tie_break='first')\n",
        "    assert row[0] == 0 and np.all(row[1:] == 1)\n",
        "\n",
        "def test_ftl_self_play_shapes():\n",
        "    A = np.array([[1,0],[0,1]], float); B = A.copy()\n",
        "    T = 10\n",
        "    r,c = ftl_self_play(A,B,T,tie_break='first',seed=42)\n",
        "    assert r.shape==(T,) and c.shape==(T,)\n",
        "\n",
        "_ok2(\"T2: expected payoff vs mix\", test_expected_row_mix)\n",
        "_ok2(\"T2: ftl_row basic\", test_ftl_row_basic)\n",
        "_ok2(\"T2: self-play shapes\", test_ftl_self_play_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba346e89",
      "metadata": {
        "id": "ba346e89"
      },
      "source": [
        "### üß© Task 3 ‚Äî Multiplicative Weights Update (MWU): opponent-given and self-play\n",
        "\n",
        "- `mwu_row(col_actions, A, eta, seed=None)` ‚Üí `(row_actions, probs_over_time)` where `probs_over_time[t] = x_t` (distribution **before** sampling round `t`).\n",
        "- `mwu_self_play(A, B, T, eta_row, eta_col, seed=None)` ‚Üí simulate both players using MWU.\n",
        "  - Returns `(row_actions, col_actions, probs_row_over_time, probs_col_over_time)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f330123",
      "metadata": {
        "id": "3f330123"
      },
      "outputs": [],
      "source": [
        "def mwu_row(col_actions, A, eta, seed=None):\n",
        "    \"\"\"MWU for the row player against realized column actions.\n",
        "    Return (row_actions, probs_over_time).\"\"\"\n",
        "    # TODO: implement\n",
        "    pass\n",
        "\n",
        "\n",
        "def mwu_self_play(A, B, T, eta_row, eta_col, seed=None):\n",
        "    \"\"\"Self-play MWU for T rounds.\n",
        "    Return (row_actions, col_actions, probs_row_over_time, probs_col_over_time).\"\"\"\n",
        "    # TODO: implement\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c4af3d",
      "metadata": {
        "id": "56c4af3d"
      },
      "outputs": [],
      "source": [
        "# === Public tests: Task 3 ===\n",
        "def _ok3(name, fn):\n",
        "    try:\n",
        "        fn(); print(f\"‚úÖ {name} ‚Äî passed\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"‚ùå {name} ‚Äî {e}\")\n",
        "\n",
        "def test_mwu_shapes():\n",
        "    A = np.array([[1,0],[0,1]], float)\n",
        "    col = [0,1,0,1]\n",
        "    row, P = mwu_row(col, A, eta=0.2, seed=631)\n",
        "    assert row.shape==(len(col),,) and P.shape==(len(col), A.shape[0])\n",
        "    assert np.allclose(P.sum(axis=1),1.0)\n",
        "\n",
        "def test_mwu_self_play_shapes():\n",
        "    A = np.array([[1,0],[0,1]], float); B = A.copy()\n",
        "    T=12\n",
        "    r,c,Pr,Pc = mwu_self_play(A,B,T,eta_row=0.2,eta_col=0.2,seed=7)\n",
        "    assert r.shape==(T,) and c.shape==(T,)\n",
        "    assert Pr.shape==(T,A.shape[0]) and Pc.shape==(T,B.shape[1])\n",
        "\n",
        "_ok3(\"T3: mwu_row shapes\", test_mwu_shapes)\n",
        "_ok3(\"T3: mwu_self_play shapes\", test_mwu_self_play_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d7931d",
      "metadata": {
        "id": "26d7931d"
      },
      "source": [
        "### üß© Task 4 ‚Äî Distance to NE for Self-Play (FTL & MWU)\n",
        "\n",
        "Given a game `(A,B)` and a target **row NE mix** `x_star`, plot the distance\n",
        "$ d_t = \\|\\bar{x}_t - x^*\\|_1 $ for the **self-play** FTL and MWU runs.\n",
        "\n",
        "- `average_strategy_over_time_row(row_actions, n_actions)` ‚Üí `(T, n_row)` cumulative averages.\n",
        "- `distance_to_NE(avg_over_time, x_star, ord='l1')` ‚Üí vector `d` of length `T`.\n",
        "- `compare_selfplay_convergence(A, B, T, x_star, eta_row, eta_col, seed=None)`\n",
        "  - Run `ftl_self_play` and `mwu_self_play` on the same game/horizon/seed.\n",
        "  - Return `{'d_ftl': d_ftl, 'd_mwu': d_mwu}` and **plot** both curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9459c6c3",
      "metadata": {
        "id": "9459c6c3"
      },
      "outputs": [],
      "source": [
        "def average_strategy_over_time_row(row_actions, n_actions):\n",
        "    \"\"\"Return cumulative average strategy over time as an array of shape (T, n_actions).\"\"\"\n",
        "    # TODO: implement\n",
        "    pass\n",
        "\n",
        "\n",
        "def distance_to_NE(avg_over_time, x_star, ord='l1'):\n",
        "    \"\"\"Return d_t = || avg_over_time[t] - x_star ||_ord over t.\"\"\"\n",
        "    # TODO: implement\n",
        "    pass\n",
        "\n",
        "\n",
        "def compare_selfplay_convergence(A, B, T, x_star, eta_row, eta_col, seed=None):\n",
        "    \"\"\"Run FTL and MWU self-play and compare distance-to-NE for the row player.\n",
        "    Return {'d_ftl': d_ftl, 'd_mwu': d_mwu}. Also plot both curves.\"\"\"\n",
        "    # TODO: implement\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3bb56ec",
      "metadata": {
        "id": "b3bb56ec"
      },
      "outputs": [],
      "source": [
        "# === Public tests: Task 4 ===\n",
        "def _ok4(name, fn):\n",
        "    try:\n",
        "        fn(); print(f\"‚úÖ {name} ‚Äî passed\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"‚ùå {name} ‚Äî {e}\")\n",
        "\n",
        "def test_avg_over_time_tiny():\n",
        "    avg = average_strategy_over_time_row([0,2,2,1,2], 3)\n",
        "    expected = np.array([\n",
        "        [1,0,0],\n",
        "        [0.5,0,0.5],\n",
        "        [1/3,0,2/3],\n",
        "        [0.25,0.25,0.5],\n",
        "        [0.2,0.2,0.6],\n",
        "    ])\n",
        "    assert np.allclose(avg, expected)\n",
        "\n",
        "def test_distance_basic():\n",
        "    avg = np.array([[1,0],[0.6,0.4],[0.5,0.5]], float)\n",
        "    x_star = np.array([0.5,0.5], float)\n",
        "    d = distance_to_NE(avg, x_star, ord='l1')\n",
        "    assert d.shape==(3,)\n",
        "    assert np.isclose(d[-1], 0.0)\n",
        "\n",
        "_ok4(\"T4: avg over time tiny\", test_avg_over_time_tiny)\n",
        "_ok4(\"T4: distance basic\", test_distance_basic)\n",
        "\n",
        "def test_matching_pennies_convergence():\n",
        "    # Row payoff matrix (col gets the negative)\n",
        "    A = np.array([[+1, -1],\n",
        "                  [-1, +1]], float)\n",
        "    B = -A\n",
        "    x_star = np.array([0.5, 0.5])\n",
        "    T = 400\n",
        "\n",
        "    out = compare_selfplay_convergence(A, B, T, x_star,\n",
        "                                       eta_row=0.1, eta_col=0.1,\n",
        "                                       seed=631)\n",
        "    d_ftl, d_mwu = out[\"d_ftl\"], out[\"d_mwu\"]\n",
        "\n",
        "    # Sanity: correct shapes\n",
        "    assert d_ftl.shape == (T,)\n",
        "    assert d_mwu.shape == (T,)\n",
        "\n",
        "    # FTL tends to cycle away from NE; MWU should average closer in tail\n",
        "    tail_ftl = np.mean(d_ftl[-100:])\n",
        "    tail_mwu = np.mean(d_mwu[-100:])\n",
        "    print(\"Avg tail distances:\", tail_ftl, tail_mwu)\n",
        "\n",
        "    assert tail_mwu < tail_ftl + 0.2, \"MWU should not be worse than FTL in MP\"\n",
        "    assert tail_mwu < 0.5, \"MWU should be reasonably close to NE on average\"\n",
        "\n",
        "test_matching_pennies_convergence()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è Methods Note (Reflection)\n",
        "\n",
        "Please add a short note (5‚Äì10 sentences) reflecting on your work for this assignment. You might address questions such as:\n",
        "\n",
        "- Which parts of the code did you find most challenging to implement, and why?\n",
        "\n",
        "- How did you verify that your implementations were correct? (e.g., reasoning, manual checks, extra experiments)\n",
        "\n",
        "- Did you notice any interesting behavior of the algorithms when you tested them?\n",
        "\n",
        "- If you had more time, how might you improve or extend your code?\n",
        "\n",
        "- What did you learn about regret minimization or equilibria through coding these tasks?\n",
        "\n",
        "This note will not be graded for correctness, but for thoughtful engagement."
      ],
      "metadata": {
        "id": "RDF5tOpih_X6"
      },
      "id": "RDF5tOpih_X6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}